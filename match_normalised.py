"""match_normalised

Match concrete match results with a more generalised version of the sandboxing profile,
then match the result with a single general sandboxing profile.
"""

import argparse
import json
import os
import operator

from misc.logger import create_logger

logger = create_logger('match_normalised')

def load_json_file(filepath):
    """Loads a JSON file from disk."""
    if not os.path.exists(filepath):
        return None

    with open(filepath) as infile:
        return json.load(infile)


def load_matches(match_file):
    """Loads matches from a file. Attempts to also load rematches from a similarly
    named file, and, if successful, replaces rematched entries in the match results
    prior to returning."""
    rematch_file = match_file.replace("match_results.json", "rematch_results.json")
    if not os.path.exists(rematch_file):
        with open(match_file) as matches:
            return json.load(matches)

    with open(match_file) as matches, open(rematch_file) as rematches:
        match_entries = json.load(matches)
        match_dict = {
            x[0]: x[1] for x in match_entries
        }
        rematch_entries = json.load(rematches)
        rematch_dict = {
            x[0]: x[1] for x in rematch_entries
        }

        # Make sure all existing inconsistent results were rematched in the rematch results
        for k, v in match_dict.items():
            if v == 'inconsistent':
                assert k in rematch_dict

        # Replace rematched values in match dictionary
        for k, v in rematch_dict.items():
            match_dict[k] = v

        return sorted(match_dict.items())


def load_normalised_profile(bundle_path):
    """Loads the normalised profile for a given bundle id."""
    normalised_profile_path = os.path.join(bundle_path, "normalised_profile.json")
    return load_json_file(normalised_profile_path)


def load_original_profile(bundle_path):
    """Loads the original profile for a given bundle id."""
    original_profile_path = os.path.join(bundle_path, "patched_profile.json")
    return load_json_file(original_profile_path)


def create_matching(base_profile : list,
                    reduced_profile : list,
                    compare_func = operator.eq) -> dict:
    """Create a matching between two (slightly) different rulesets.

    Precondition: len(base_profile) >= len(reduced_profile)

    :param base_profile The unchanged base profile. Assumed to have at least as many rules as
                        the reduced profile
    :param reduced_profile The "reduced" profile, i.e. the normalised version. This reduced
                           profile commonly has exactly as many rules as the original profile.
                           However, in some cases it is possible that the reduced profile has
                           less rules, as application specific rules are generated by the Sandbox.
    :param compare_func The comparison function to use to compare two sandbox rules with one another.
    :returns dictionary mapping rule i of the base profile to rule j of the reduced profile, or -1
             if the mapping failed.
    """
    assert isinstance(base_profile, list)
    assert isinstance(reduced_profile, list)
    assert len(base_profile) >= len(reduced_profile)

    matched_indices = set()
    matches = {}

    if base_profile is None or reduced_profile is None:
        logger.error("Cannot create matching: inputs missing.")
        return

    for i, rule in enumerate(base_profile):
        # Find matching rule in reduced_profile

        # Base profile and reduced profile are related in the sense
        # that the reduced profile is based off of the base profile.
        # As such, and by studying the sandbox profile generation algorithm,
        # we can say that rule i will only ever match with rule j in the reduced
        # profile if j <= i, i.e. i cannot match with rule j if j >> i.
        # Therefore, we stop iteration at the current rule, because otherwise we get
        # nonsensical matches where the application matches with an entirely
        # unrelated rule at the back of the ruleset.
        for j, other_rule in enumerate(reduced_profile[0:i+1]):
            # Make sure not to match a rule twice.
            if compare_func(rule, other_rule) and j not in matched_indices:
                matches[i] = j
                matched_indices.add(j)
                break
        else:
            matches[i] = -1

    return matches


def match_normalised_to_generic(normalised, generic):
    return create_matching(normalised, generic)


def match_application_to_normalised(application, normalised):
    comparator = lambda x, y: (x['action'] == y['action']) and (x['operations'] == y['operations'])

    return create_matching(application, normalised, comparator)


def group_consecutives(vals, step=1):
    """Return list of consecutive lists of numbers from vals (number list)."""
    run = []
    result = [run]
    expect = None
    for v in vals:
        if (v == expect) or (expect is None):
            run.append(v)
        else:
            run = [v]
            result.append(run)
        expect = v + step
    return result


def display_matches(matches, normalised, general):
    matches = [matches[k] for k in sorted(matches.keys())]

    grouped_matches = group_consecutives(matches)
    overall_index = 0

    for group in grouped_matches:
        print("{} -> {}".format(range(overall_index, overall_index + len(group)), group))
        if group[0] != -1:
            print("{} -> {}".format(normalised[overall_index], general[group[0]]))
        overall_index += len(group)


def match_rules(bundle_path, general_profile):
    """Computes an aggregate matching relation, mapping rules from the application profile
    to the final generic profile."""
    # Load inputs
    original = load_original_profile(bundle_path)
    normalised = load_normalised_profile(bundle_path)

    if original is None or normalised is None:
        return None

    # Compute matching between app profile and generic version
    first_matching = match_application_to_normalised(original, normalised)
    # Compute matching between generic app profile and generic profile
    second_matching = match_normalised_to_generic(normalised, general_profile)

    # return aggregate matching a -> c where a -> b -> c and b is omitted.
    return {
        k: -1 if v == -1 else second_matching[v] for k, v in first_matching.items()
    }


def process_entry(bundle_path, general_profile):
    """Process and transform collected matching results into a form that is usable for
    the generic profile we use for visualisation at the end."""
    print('Processing {}'.format(bundle_path))

    MATCH_RESULTS_AT = os.path.join(bundle_path, "match_results.json")
    SANDBOX_PROFILE_AT = os.path.join(bundle_path, "patched_profile.json")
    NORMALISED_PROFILE_AT = os.path.join(bundle_path, "normalised_profile.json")

    if not os.path.exists(MATCH_RESULTS_AT) or not os.path.exists(SANDBOX_PROFILE_AT) or not os.path.exists(NORMALISED_PROFILE_AT):
        logger.error("Results at path {} lack required information and will not be processed.".format(bundle_path))
        return

    matches = load_matches(MATCH_RESULTS_AT)
    rule_mapping = match_rules(bundle_path, general_profile)

    # Applying the rule mapping to the matches to get general_matches
    general_matches = {
        k: rule_mapping[v] if v != "inconsistent" else -1 for k, v in matches
    }

    last_path_component = os.path.split(bundle_path)[1]
    with open(os.path.join("/tmp/match_final/", last_path_component), "w") as outfile:
        json.dump(general_matches, outfile, indent=4)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--match-results', required=True,
                        help='Base folder containing matching results')
    parser.add_argument('--general-profile', required=True,
                        help='The general profile with which to ultimately match the results.')
    args = parser.parse_args()

    general_profile = load_json_file(args.general_profile)

    basedir = args.match_results
    entries = [ os.path.join(basedir, x) for x in os.listdir(basedir) ]
    for entry in entries:
        process_entry(entry, general_profile)

if __name__ == '__main__':
    main()