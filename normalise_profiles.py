import json
import argparse
import tempfile
import os
import plistlib
import operator
import multiprocessing
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "maap"))

from maap.misc.plist import parse_resilient
from maap.misc.logger import create_logger
from maap.extern.tools import call_sbpl

logger = create_logger('normalise_profiles')


def normalise_container_metadata(metadata):
    """Normalises existing container metadata and returns a normalised version
    back to the user.

    :param metadata Dictionary containing the original Container metadata."""

    # Container metadata contains a number of important keys.
    # We only want to change the SandboxProfileDataValidationInfo entries
    relevant_entries = metadata.get('SandboxProfileDataValidationInfo', None)
    assert relevant_entries is not None

    # SandboxProfileDataValidationParametersKey contains general parameters for
    # sandbox evaluation such as HOME_DIR, USER, app specific information, ...
    sandbox_parameters = relevant_entries.get('SandboxProfileDataValidationParametersKey', None)
    assert sandbox_parameters is not None and isinstance(sandbox_parameters, dict)

    # Grab home directory, we need that later.
    home_dir = sandbox_parameters.get('_HOME')

    # Change values for all keys to placeholders.
    for key in sandbox_parameters.keys():
        # For each key, we simply use the key in uppercase, along delimiters as a placeholder.
        sandbox_parameters[key] = "$" + key.upper() + "$"

    # SandboxProfileDataValidationRedirectablePathsKey contains redirectable paths that are part
    # of the user's home directory. Patch these paths such that the HOME placeholder is used instead.
    redirectable_paths = [x.replace(home_dir, sandbox_parameters['_HOME'])
                          for x
                          in relevant_entries.get('SandboxProfileDataValidationRedirectablePathsKey', [])]
    assert redirectable_paths is not None and isinstance(redirectable_paths, list)

    # Store the modified values back into the dictionary
    relevant_entries['SandboxProfileDataValidationParametersKey'] = sandbox_parameters
    relevant_entries['SandboxProfileDataValidationRedirectablePathsKey'] = redirectable_paths

    metadata['SandboxProfileDataValidationInfo'] = relevant_entries

    return metadata


def profile_for_metadata(metadata, format='scheme', patch=False):
    with tempfile.TemporaryDirectory() as tempdir:
        container_metadata = os.path.join(tempdir, 'Container.plist')
        # Write normalised container metadata to file
        with open(container_metadata, 'wb') as outfile:
            plistlib.dump(metadata, outfile)

        return call_sbpl(tempdir, format, patch)


def normalised_profile_for_metadata(metadata, format='scheme', patch=False):
    """Create a normalised bundle for the given metadata.

    :param metadata The original metadata of the container
    :param format The format of the output. Possible values are scheme and json.
    :returns Profile data for the normalised profile"""

    normalised_metadata = normalise_container_metadata(metadata)
    normalised_profile = profile_for_metadata(normalised_metadata, format=format, patch=patch)
    return normalised_profile


def create_matching(base_profile : list, 
                    reduced_profile : list, 
                    compare_func = operator.eq) -> dict:
    """Create a matching between two (slightly) different rulesets.

    @param base_profile The unchanged base profile. Assumed to have at least as many rules as
                        the reduced profile
    @param reduced_profile The "reduced" profile, i.e. the normalised version. This reduced
                           profile commonly has exactly as many rules as the original profile.
                           However, in some cases it is possible that the reduced profile has
                           less rules, as application specific rules are generated by the Sandbox.
    @returns dictionary mapping rule i of the base profile to rule j of the reduced profile, or -1
             if the mapping failed.
    @pre len(base_profile) >= len(reduced_profile)
    """
    assert isinstance(base_profile, list)
    assert isinstance(reduced_profile, list)
    assert len(base_profile) >= len(reduced_profile)

    matched_indices = set()
    matches = {}

    if base_profile is None or reduced_profile is None:
        logger.error("Cannot create matching: inputs missing.")
        return

    for i, rule in enumerate(base_profile):
        # Find matching rule in reduced_profile

        # Base profile and reduced profile are related in the sense
        # that the reduced profile is based off of the base profile.
        # As such, and by studying the sandbox profile generation algorithm,
        # we can say that rule i will only ever match with rule j in the reduced
        # profile if j <= i, i.e. i cannot match with rule j if j >> i.
        # Therefore, we stop iteration at the current rule, because otherwise we get
        # nonsensical matches where the application matches with an entirely
        # unrelated rule at the back of the ruleset.
        for j, other_rule in enumerate(reduced_profile[0:i+1]):
            # Make sure not to match a rule twice.
            if compare_func(rule, other_rule) and j not in matched_indices:
                matches[i] = j
                matched_indices.add(j)
                break
        else:
            matches[i] = -1

    return matches


def match_normalised_to_generic(normalised, generic):
    return create_matching(normalised, generic)


def match_application_to_normalised(application, normalised):
    comparator = lambda x, y: (x['action'] == y['action']) and (x['operations'] == y['operations'])

    return create_matching(application, normalised, comparator)


def check_normalised_profile(normalised_profile, original_profile):
    """Sanity checks a given normalised profile.

    Verifies that its length and its operations and actions match a given original profile."""
    # Check lengths
    matching = match_application_to_normalised(original_profile, normalised_profile)

    if len(original_profile) == len(normalised_profile):
        # Special case, make sure matching is 1:1
        for k, v in matching.items():
            if k != v:
                return False
    else:
        print('Profile needs actual matching: {}'.format(matching))

    return True


def load_file(at):
    if os.path.exists(at):
        if at.endswith(".plist"):
            # Load plist
            return parse_resilient(at)
        elif at.endswith(".json"):
            with open(at) as infile:
                return json.load(infile)
    return None


def already_normalised(match_results, bundle_id):
    path = os.path.join(match_results, bundle_id, "normalised_profile.json")
    return os.path.exists(path)


def process_bundle_id(bundle_id, match_results, containers):
    """Processes results for a single bundle id.
    This function looks up the original sandbox profile,
    normalises its container metadata and re-generates a new sandbox profile,
    which is then undergoing basic sanity checking and finally returned
    to the user

    :param bundle_id The bundle id of the application of interest
    :param match_results Base folder for match results. Is used to read existing results
    and to verify new results.
    :param containers Base folder for Container.plist metadata information.

    :returns New file containing generalised profile."""
    print("Processing {}".format(bundle_id))
    if already_normalised(match_results, bundle_id):
        logger.info("Skipping processing {}, as bundle has already been processed.".format(bundle_id))
        return

    metadata = load_file(os.path.join(containers, bundle_id, "Container.plist"))
    if not metadata:
        logger.error("Metadata for bundle id {} does not exist or could not be loaded.".format(bundle_id))
        return

    original_profile = load_file(os.path.join(match_results, bundle_id, "patched_profile.json"))
    if not original_profile:
        logger.error("Could not find or load existing sandbox profile for bundle id {}".format(bundle_id))
        return

    profile = profile_for_metadata(metadata, format='json')
    if profile is None:
        logger.error("Failed to get profile for metadata for bundle id {}".format(bundle_id))
        return
    norm_profile = normalised_profile_for_metadata(metadata, format='json')
    if norm_profile is None:
        logger.error("Failed to get normalised profile for metadata for bundle id {}".format(bundle_id))
        return

    recreated_profile  = json.loads(profile)
    normalised_profile = json.loads(norm_profile)

    assert isinstance(recreated_profile, list)
    assert isinstance(normalised_profile, list)

    if original_profile != recreated_profile:
        logger.error("Failed to recreate profiles for bundle id {}".format(bundle_id))
        return

    # Write out normalised profile
    with open(os.path.join(match_results, bundle_id, "normalised_profile.json"), "w") as outfile:
        json.dump(normalised_profile, outfile, indent=4)

    if not check_normalised_profile(normalised_profile, recreated_profile):
        logger.error("Sanity checks for normalised profile for bundle id {} failed.".format(bundle_id))
    else:
        logger.info("Successfully processed data for bundle id {}".format(bundle_id))


_match_results = None
_containers = None


def process_entry(bundle_id):
    return process_bundle_id(bundle_id, _match_results, _containers)


def process_entries(entries):
    # Not a swimming pool :-(
    pool = multiprocessing.Pool(4)

    pool.map(process_entry, entries)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--match-results', required=True,
                        help='Base folder containing matching results')
    parser.add_argument('--containers', required=True,
                        help='Base folder containing Container.plist metadata information.')
    args = parser.parse_args()

    container_folder = args.containers
    match_results = args.match_results

    bundle_ids = [x for x in os.listdir(match_results) if not x.startswith('.')]

    global _match_results
    global _containers

    _match_results = match_results
    _containers = container_folder

    process_entries(bundle_ids)


if __name__ == "__main__":
    main()