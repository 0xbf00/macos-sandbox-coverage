"""
Module to calculate a mapping between original and normalised and generalised
sandbox proiles.
"""
import json

from typing import Any, Dict, List, Optional, Tuple


def create_matching(
    base_profile: List[Dict[str, Any]],
    reduced_profile: List[Dict[str, Any]],
    replacements: Optional[Dict[str, str]] = None,
) -> Dict[int, int]:
    """
    Create a matching between two (slightly) different rulesets.

    Precondition: len(base_profile) >= len(reduced_profile)

    :param base_profile
        The unchanged base profile. Assumed to have at least as many rules as
        the reduced profile
    :param reduced_profile
        The "reduced" profile, i.e. the normalised version. This reduced
        profile commonly has exactly as many rules as the original profile.
        However, in some cases it is possible that the reduced profile has
        fewer rules, as application specific rules are generated by the
        Sandbox.
    :param replacements
        The replacements used for reducing the profile. If None, no
        replacements will be performed.
    :returns dictionary mapping rule i of the base profile to rule j of the
        reduced profile.
    """

    assert len(base_profile) >= len(reduced_profile)

    matches: Dict[int, int] = {}
    last_matched_idx = -1

    for base_idx, base_rule in enumerate(base_profile):
        base = json.dumps(base_rule, sort_keys=True)

        if len(reduced_profile) <= last_matched_idx:
            # Nothing left to match in the reduced profile
            break

        # Since rules are order-preserving, we only need to look forward from
        # the last_matched rule onwards.
        for reduced_idx, reduced_rule in enumerate(
            reduced_profile[last_matched_idx + 1:],
            start=last_matched_idx + 1,
        ):
            reduced = json.dumps(reduced_rule, sort_keys=True)

            if replacements is not None:
                # Undo replacements in order to match the correct rule
                for key, value in replacements.items():
                    reduced = reduced.replace(key, value)

            if base == reduced:
                matches[base_idx] = reduced_idx
                last_matched_idx = reduced_idx
                break

    return matches


def generalise_results(state: Dict[str, Any]) -> Tuple[bool, dict]:
    """
    Process and transform collected matching results into a form that is usable
    for the generic profile we use for visualisation at the end.
    """

    original = json.loads(state['sandbox_profiles']['original'])
    normalised = state['sandbox_profiles']['normalised']
    general = state['sandbox_profiles']['general']
    replacements: Dict[str, str] = state['normalisation_replacements']

    # Compute first matching between app profile and generic version
    # Because the normalised sandbox profile uses generic placeholder values
    # for things such as paths, we only compare the action (allow | deny) and
    # the involved operations and ignore any filters
    original_to_normalised = create_matching(
        original,
        normalised,
        replacements,
    )

    # Compute matching between generic app profile and generic profile
    # Both the normalised and the general profile use the same placeholders for
    # things such as paths. As such, we compare the entire rule, including
    # filters and modifiers
    normalised_to_generalised = create_matching(normalised, general)

    state['rule_mapping'] = {
        'original_to_normalised': original_to_normalised,
        'normalised_to_generalised': normalised_to_generalised,
    }

    return True, state